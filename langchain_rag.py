import streamlit as st
from langchain_core.documents import Document
from unstructured.partition.pdf import partition_pdf
from unstructured.chunking.title import chunk_by_title
import tempfile
import ast # Import the Abstract Syntax Tree module
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.vectorstores import FAISS
from langchain.tools.retriever import create_retriever_tool
from langchain.agents import AgentExecutor, create_tool_calling_agent
from langchain_community.embeddings import DashScopeEmbeddings
from langchain.chat_models import init_chat_model
import os
import time
from dotenv import load_dotenv

# Import tools and functions from our new module
from academic_search import (
    extract_title_and_initial_text,
    summarize_text_to_abstract,
    generate_search_keywords,
    search_semantic_scholar,
)

load_dotenv(override=True)

# è®¾ç½®Hugging Faceé•œåƒï¼Œè§£å†³æ¨¡å‹ä¸‹è½½é—®é¢˜
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

DeepSeek_API_KEY = os.getenv("DEEPSEEK_API_KEY")
dashscope_api_key = os.getenv("dashscope_api_key")

os.environ["KMP_DUPLICATE_LIB_OK"]="TRUE"


def inject_custom_css():
    st.markdown("""
        <style>
            /* General App Styling */
            .stApp {
                background-color: #F0F2F6; /* A light, neutral background */
            }

            /* Main Title */
            h1 {
                color: #262730;
                font-family: "Source Sans Pro", sans-serif;
                font-weight: 700;
                text-align: center;
                padding-bottom: 10px;
                margin-bottom: 20px;
                border-bottom: 2px solid #D1D5DB;
            }

            /* Sidebar styling */
            [data-testid="stSidebar"] {
                background-color: #FFFFFF;
                border-right: 1px solid #E5E7EB;
            }
            [data-testid="stSidebar"] h1 {
                font-size: 24px;
                text-align: left;
                border-bottom: none;
                padding-bottom: 0;
                margin-bottom: 15px;
            }

            /* Custom button styling */
            .stButton>button {
                border: 1px solid #BCCCDC; /* Softer border color */
                border-radius: 8px;
                color: #263238; /* Darker text for readability */
                background-color: #FFFFFF;
                padding: 10px 24px;
                font-weight: 600;
                transition: all 0.2s ease-in-out;
            }
            .stButton>button:hover {
                border-color: #374151;
                color: #FFFFFF;
                background-color: #4A5568; /* A cool gray on hover */
            }
            .stButton>button:disabled {
                border: 1px solid #D1D5DB;
                color: #9CA3AF;
                background-color: #F3F4F6;
            }
            
            /* Main action button in sidebar */
            [data-testid="stSidebar"] .stButton[data-testid="stButton"] button {
                 background-color: #374151;
                 color: white;
                 width: 100%;
            }
             [data-testid="stSidebar"] .stButton[data-testid="stButton"] button:hover {
                 background-color: #262730;
             }

            /* Text Input Box */
            .stTextInput>div>div>input {
                border: 1px solid #D1D5DB;
                background-color: #FFFFFF;
                border-radius: 8px;
                padding: 12px;
                font-size: 16px;
            }
            .stTextInput>div>div>input:focus {
                border: 2px solid #374151;
                box-shadow: none;
            }
            
            /* Expander (Usage Guide) */
            .stExpander {
                border: 1px solid #E5E7EB;
                box-shadow: none;
                border-radius: 8px;
                background-color: #FFFFFF;
            }
            .stExpander header {
                font-size: 16px;
                font-weight: 600;
                color: #374151;
            }

            /* Status boxes (Success, Warning, Info) */
            [data-testid="stAlert"] {
                border-radius: 8px;
                padding-left: 20px;
                background-color: #FFFFFF;
                border-left: 5px solid;
            }
            [data-testid="stAlert"][data-baseweb="notification-positive"] {
                border-color: #34D399;
                color: #065F46;
            }
            [data-testid="stAlert"][data-baseweb="notification-warning"] {
                border-color: #FBBF24;
                color: #92400E;
            }
            [data-testid="stAlert"][data-baseweb="notification-info"] {
                border-color: #60A5FA;
                color: #1E40AF;
            }
        </style>
    """, unsafe_allow_html=True)


embeddings = DashScopeEmbeddings(
    model="text-embedding-v1", dashscope_api_key=dashscope_api_key
)

def process_pdfs(pdf_docs):
    """
    ä½¿ç”¨unstructuredåº“å¤„ç†PDFæ–‡ä»¶ï¼Œè¿›è¡Œç»“æ„åŒ–åˆ‡åˆ†å¹¶å­˜å…¥å‘é‡æ•°æ®åº“ã€‚
    æ­¤ç‰ˆæœ¬ä½¿ç”¨ 'chunk_by_title' ç­–ç•¥æ¥åˆ›å»ºæ›´ç¬¦åˆé€»è¾‘ã€æ›´å¤§çš„å—ã€‚
    """
    all_docs = []
    for pdf in pdf_docs:
        # Unstructuredéœ€è¦ä¸€ä¸ªæ–‡ä»¶è·¯å¾„ã€‚æˆ‘ä»¬å°†ä¸Šä¼ çš„æ–‡ä»¶ä¿å­˜åˆ°ä¸€ä¸ªä¸´æ—¶æ–‡ä»¶ä¸­ã€‚
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
            tmp.write(pdf.getvalue())
            tmp_path = tmp.name
        
        # 1. ä½¿ç”¨unstructuredçš„partition_pdfæ™ºèƒ½æå–å…ƒç´ 
        # strategy="hi_res" ä½¿ç”¨é«˜ç²¾åº¦æ¨¡å‹ï¼Œèƒ½æ›´å¥½åœ°è¯†åˆ«æ ‡é¢˜ã€è¡¨æ ¼ç­‰ï¼Œä½†éœ€è¦æ›´å¤šè®¡ç®—èµ„æºï¼Œä¸”éœ€è¦ä¸‹è½½æ¨¡å‹
        elements = partition_pdf(tmp_path, strategy="fast", infer_table_structure=True)
        
        # 2. ä½¿ç”¨chunk_by_titleç­–ç•¥å°†å…ƒç´ åˆ†å—
        # è¿™ä¸ªç­–ç•¥ä¼šæ ¹æ®æ–‡æ¡£ä¸­çš„æ ‡é¢˜ï¼ˆ<h1>, <h2>ç­‰ï¼‰æ¥ç»„ç»‡å†…å®¹ï¼Œéå¸¸é€‚åˆå­¦æœ¯è®ºæ–‡
        # max_characters æ§åˆ¶æ¯ä¸ªå—çš„æœ€å¤§å­—ç¬¦æ•°ï¼Œä»¥é€‚åº”æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£
        chunks = chunk_by_title(
            elements,
            max_characters=2000,      # å¢åŠ æ¯ä¸ªå—çš„æœ€å¤§å­—ç¬¦æ•°
            new_after_n_chars=1500,   # åœ¨æ­¤å­—ç¬¦æ•°åå€¾å‘äºåˆ›å»ºæ–°å—
            combine_text_under_n_chars=500  # åˆå¹¶å°äºæ­¤å­—ç¬¦æ•°çš„æ–‡æœ¬å—
        )

        # 3. å°†åˆ†å—ç»“æœè½¬æ¢ä¸ºLangChainçš„Documentå¯¹è±¡
        for chunk in chunks:
            # å°†unstructuredçš„Elementå¯¹è±¡è½¬æ¢ä¸ºdict
            metadata = chunk.metadata.to_dict()
            # ç§»é™¤ä¸€äº›ä¸éœ€è¦æˆ–å¯èƒ½å¯¼è‡´é—®é¢˜çš„å…ƒæ•°æ®
            metadata.pop('coordinates', None)
            metadata.pop('parent_id', None)
            # æ·»åŠ æ–‡ä»¶æ¥æºä¿¡æ¯
            metadata['source'] = pdf.name
            
            all_docs.append(Document(page_content=chunk.text, metadata=metadata))
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.remove(tmp_path)
    
    # åœ¨å¤„ç†å®Œæ‰€æœ‰PDFåï¼Œåˆ›å»ºå‘é‡æ•°æ®åº“
    if not all_docs:
        raise ValueError("æ— æ³•ä»PDFæ–‡ä»¶ä¸­æå–ä»»ä½•å†…å®¹ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆã€‚")
        
    vector_store(all_docs)
    return len(all_docs)


def vector_store(documents):
    # loaderç›´æ¥æä¾›äº†Documentå¯¹è±¡ï¼Œå› æ­¤æˆ‘ä»¬ä½¿ç”¨from_documents
    vector_store = FAISS.from_documents(documents, embedding=embeddings)
    vector_store.save_local("faiss_db")

def get_conversational_chain(tools, ques):
    llm = init_chat_model("deepseek-chat", model_provider="deepseek")
    prompt = ChatPromptTemplate.from_messages([
        (
            "system",
            """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å­¦æœ¯ç ”ç©¶åŠ©ç†ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡ï¼ˆå­¦æœ¯è®ºæ–‡ï¼‰ï¼Œç²¾å‡†ã€æ¸…æ™°åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚è¯·ä¸¥æ ¼éµå®ˆä»¥ä¸‹æŒ‡å—ï¼š
1.  **ç»¼åˆä¿¡æ¯**ï¼šä»æ–‡æ¡£çš„å„ä¸ªéƒ¨åˆ†ç»¼åˆä¿¡æ¯ï¼Œæä¾›å…¨é¢è€Œæ·±å…¥çš„å›ç­”ï¼Œè€Œä¸ä»…ä»…æ˜¯åˆ—å‡ºæ‰¾åˆ°çš„ç‰‡æ®µã€‚
2.  **ç²¾å‡†å…·ä½“**ï¼šå½“è¢«é—®åŠç ”ç©¶æ–¹æ³•ã€åˆ›æ–°ç‚¹ã€ç»“è®ºæˆ–æ•°æ®æ—¶ï¼Œæä¾›å…·ä½“ç»†èŠ‚å’Œæ–‡æœ¬ä¸­çš„è¯æ®ã€‚
3.  **æ‰¿è®¤å±€é™**ï¼šå¦‚æœè®ºæ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯æˆ–è¡¨è¿°æ¨¡ç³Šï¼Œè¯·æ˜ç¡®æŒ‡å‡ºï¼Œä¸è¦æœæ’°ç­”æ¡ˆã€‚
4.  **ç»“æ„åŒ–å›ç­”**ï¼šå¯¹äºå¤æ‚é—®é¢˜ï¼Œä½¿ç”¨è¦ç‚¹ã€åˆ—è¡¨æˆ–æ®µè½æ¥ç»„ç»‡å›ç­”ï¼Œä»¥æé«˜å¯è¯»æ€§ã€‚
5.  **å¿ äºåŸæ–‡**ï¼šæ‰€æœ‰å›ç­”éƒ½å¿…é¡»ä¸¥æ ¼åŸºäºæ‰€æä¾›çš„PDFæ–‡æ¡£å†…å®¹ã€‚å¦‚æœç­”æ¡ˆä¸åœ¨ä¸Šä¸‹æ–‡ä¸­ï¼Œè¯·è¯´â€œç­”æ¡ˆä¸åœ¨æä¾›çš„æ–‡çŒ®ä¸­â€ã€‚""",
        ),
        ("placeholder", "{chat_history}"),
        ("human", "{input}"),
        ("placeholder", "{agent_scratchpad}"),
    ])
    
    tool = [tools]
    agent = create_tool_calling_agent(llm, tool, prompt)
    agent_executor = AgentExecutor(agent=agent, tools=tool, verbose=True)
    
    response = agent_executor.invoke({"input": ques})
    print(response)
    st.write("ğŸ¤– å›ç­”: ", response['output'])

def check_database_exists():
    """æ£€æŸ¥FAISSæ•°æ®åº“æ˜¯å¦å­˜åœ¨"""
    return os.path.exists("faiss_db") and os.path.exists("faiss_db/index.faiss")

def user_input(user_question):
    # æ£€æŸ¥æ•°æ®åº“æ˜¯å¦å­˜åœ¨
    if not check_database_exists():
        st.error("âŒ è¯·å…ˆä¸Šä¼ PDFæ–‡ä»¶å¹¶ç‚¹å‡»'Submit & Process'æŒ‰é’®æ¥å¤„ç†æ–‡æ¡£ï¼")
        st.info("ğŸ’¡ æ­¥éª¤ï¼š1ï¸âƒ£ ä¸Šä¼ PDF â†’ 2ï¸âƒ£ ç‚¹å‡»å¤„ç† â†’ 3ï¸âƒ£ å¼€å§‹æé—®")
        return
    
    try:
        # åŠ è½½FAISSæ•°æ®åº“
        new_db = FAISS.load_local("faiss_db", embeddings, allow_dangerous_deserialization=True)
        # å¯ä»¥ä¿®æ”¹top_kçš„å€¼ï¼Œæ¥æ§åˆ¶è¿”å›çš„æ–‡æ¡£æ•°é‡ï¼Œé»˜è®¤æ˜¯4ã€‚ä¸ªäººæ¨èk=6çš„æ•ˆæœæ¯”è¾ƒå¥½
        retriever = new_db.as_retriever(search_kwargs={'k': 6})
        retrieval_chain = create_retriever_tool(retriever, "pdf_extractor", "This tool is to give answer to queries from the pdf")
        get_conversational_chain(retrieval_chain, user_question)
        
    except Exception as e:
        st.error(f"âŒ åŠ è½½æ•°æ®åº“æ—¶å‡ºé”™: {str(e)}")
        st.info("è¯·é‡æ–°å¤„ç†PDFæ–‡ä»¶")


def run_related_papers_agent(pdf_file):
    """
    è¿è¡Œä¸€ä¸ªAgentæ¥æŸ¥æ‰¾ä¸ç»™å®šPDFç›¸å…³çš„æ–‡çŒ®ã€‚
    """
    with st.spinner("æ­£åœ¨æå–è®ºæ–‡æ ¸å¿ƒä¿¡æ¯..."):
        try:
            details = extract_title_and_initial_text(pdf_file)
            title = details.get("title")
            initial_text = details.get("initial_text")

            if title == "Unknown Title":
                st.error("æ— æ³•ä»æ­¤PDFä¸­è¯†åˆ«å‡ºæ ‡é¢˜ï¼Œè¯·å°è¯•å…¶ä»–æ–‡ä»¶ã€‚")
                return
            st.info(f"ä»¥è®ºæ–‡ **ã€Š{title}ã€‹** ä¸ºåŸºç¡€è¿›è¡Œåˆ†æ...")

        except Exception as e:
            st.error(f"è§£æPDFæ—¶å‡ºé”™: {e}")
            return

    with st.spinner("AIæ­£åœ¨ç”Ÿæˆæ£€ç´¢ç­–ç•¥å¹¶æŸ¥æ‰¾ç›¸å…³æ–‡çŒ®..."):
        try:
            # 1. å®šä¹‰å¯ç”¨çš„å·¥å…·
            tools = [summarize_text_to_abstract, generate_search_keywords, search_semantic_scholar]

            # 2. åˆ›å»ºLLM
            llm = init_chat_model("deepseek-chat", model_provider="deepseek")

            # 3. åˆ›å»ºAgentçš„æç¤ºè¯ (åŒ…å«æ–°çš„ä¸‰æ­¥å·¥ä½œæµ)
            prompt = ChatPromptTemplate.from_messages([
                ("system", """ä½ æ˜¯ä¸€ä¸ªå¼ºå¤§çš„ç§‘ç ”åŠ©ç†ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®æä¾›çš„è®ºæ–‡æ ‡é¢˜å’Œåˆå§‹æ–‡æœ¬ï¼Œæ‰¾åˆ°ç›¸å…³çš„å­¦æœ¯æ–‡çŒ®ã€‚ä½ å¿…é¡»ä¸¥æ ¼éµå¾ªä»¥ä¸‹æ­¥éª¤ï¼š
1.  **ç¬¬ä¸€æ­¥ï¼šæ€»ç»“æ‘˜è¦**ã€‚ä½¿ç”¨`summarize-text-to-abstract`å·¥å…·ï¼Œä¸ºæä¾›çš„`initial_text`ç”Ÿæˆä¸€æ®µç®€æ´çš„æ‘˜è¦ã€‚
2.  **ç¬¬äºŒæ­¥ï¼šæç‚¼å…³é”®è¯**ã€‚ä½¿ç”¨`generate-search-keywords`å·¥å…·ï¼Œå¹¶ä¸ºå…¶æä¾›åŸå§‹çš„`title`å’Œä½ åˆšåˆšåœ¨ç¬¬ä¸€æ­¥ç”Ÿæˆçš„`abstract`ã€‚
3.  **ç¬¬ä¸‰æ­¥ï¼šæœç´¢æ–‡çŒ®**ã€‚ä½¿ç”¨`search-semantic-scholar`å·¥å…·ï¼Œå¹¶ä¸ºå…¶æä¾›ä½ åœ¨ç¬¬äºŒæ­¥ç”Ÿæˆçš„`keywords`ã€‚
4.  **æœ€å**ï¼Œä½ å¿…é¡»å°†`search-semantic-scholar`å·¥å…·è¿”å›çš„åŸå§‹Pythonåˆ—è¡¨ï¼ˆlist of dictionariesï¼‰ä½œä¸ºä½ çš„æœ€ç»ˆç­”æ¡ˆã€‚ä¸è¦å¯¹å®ƒè¿›è¡Œä»»ä½•æ ¼å¼åŒ–ã€è½¬æ¢æˆ–æ·»åŠ ä»»ä½•è§£é‡Šæ€§æ–‡å­—ã€‚å¿…é¡»ç›´æ¥è¿”å›åŸå§‹çš„åˆ—è¡¨ç»“æ„ã€‚"""),
                ("human", "è¯·ä¸ºä»¥ä¸‹æ–‡æ¡£æŸ¥æ‰¾ç›¸å…³æ–‡çŒ®ï¼š\n\næ ‡é¢˜: {title}\n\nåˆå§‹æ–‡æœ¬: {initial_text}"),
                ("placeholder", "{agent_scratchpad}"),
            ])

            # 4. åˆ›å»ºAgent
            agent = create_tool_calling_agent(llm, tools, prompt)
            agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

            # 5. è¿è¡ŒAgent
            response = agent_executor.invoke({
                "title": title,
                "initial_text": initial_text
            })
            
            # 6. è§£æå¹¶æ˜¾ç¤ºç»“æœ (å…³é”®ä¿®æ­£)
            output = response.get('output')
            
            if isinstance(output, str):
                try:
                    # AIå¯èƒ½è¿”å›ä¸€ä¸ªåˆ—è¡¨çš„å­—ç¬¦ä¸²è¡¨ç¤ºï¼Œæˆ‘ä»¬å®‰å…¨åœ°å°†å…¶è§£æä¸ºçœŸå®çš„åˆ—è¡¨
                    st.session_state.related_papers = ast.literal_eval(output)
                except (ValueError, SyntaxError):
                    # å¦‚æœè§£æå¤±è´¥ï¼Œè¯´æ˜å®ƒä¸æ˜¯ä¸€ä¸ªåˆæ³•çš„åˆ—è¡¨å­—ç¬¦ä¸²
                    print(f"Agentè¿”å›äº†æ— æ³•è§£æçš„å­—ç¬¦ä¸²: {output}")
                    st.session_state.related_papers = [] # è®¾ä¸ºç©ºåˆ—è¡¨ä»¥æ˜¾ç¤ºå¤±è´¥ä¿¡æ¯
            elif isinstance(output, list):
                # AIæ­£ç¡®åœ°è¿”å›äº†ä¸€ä¸ªåˆ—è¡¨
                st.session_state.related_papers = output
            else:
                # ä»»ä½•å…¶ä»–æ„å¤–ç±»å‹
                print(f"Agentè¿”å›äº†æ„å¤–çš„ç±»å‹: {type(output)}")
                st.session_state.related_papers = []
        
        except Exception as e:
            st.error(f"æŸ¥æ‰¾ç›¸å…³æ–‡çŒ®æ—¶å‡ºé”™: {e}")
            st.session_state.related_papers = [] # åœ¨å‘ç”Ÿå¼‚å¸¸æ—¶ä¹Ÿç¡®ä¿æ¸…ç©º


def main():
    st.set_page_config("ğŸ“š å­¦æœ¯æ–‡çŒ®åˆ†æåŠ©æ‰‹", layout="wide")
    inject_custom_css()
    
    st.title("ğŸ”¬ å­¦æœ¯æ´å¯Ÿå¼•æ“")
    
    # æ˜¾ç¤ºæ•°æ®åº“çŠ¶æ€
    col1, col2 = st.columns([3, 1])
    
    with col1:
        if check_database_exists():
            st.success("âœ… çŸ¥è¯†åº“å·²å°±ç»ªï¼Œå¯ä»¥å¼€å§‹æé—®ã€‚")
        else:
            st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ å¹¶å¤„ç†PDFæ–‡ä»¶ä»¥æ„å»ºçŸ¥è¯†åº“ã€‚")
    
    with col2:
        if st.button("ğŸ—‘ï¸ æ¸…é™¤çŸ¥è¯†åº“"):
            try:
                import shutil
                if os.path.exists("faiss_db"):
                    shutil.rmtree("faiss_db")
                st.success("çŸ¥è¯†åº“å·²æ¸…é™¤ï¼")
                time.sleep(3) # æš‚åœ3ç§’
                st.rerun()
            except Exception as e:
                st.error(f"æ¸…é™¤å¤±è´¥: {e}")

    # ç”¨æˆ·é—®é¢˜è¾“å…¥
    user_question = st.text_input("ğŸ’¬ é’ˆå¯¹æ–‡çŒ®ï¼Œæå‡ºæ‚¨çš„é—®é¢˜", 
                                placeholder="ä¾‹å¦‚ï¼šæ€»ç»“ä¸€ä¸‹è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒè´¡çŒ®ã€‚è¿™ç¯‡è®ºæ–‡çš„åˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ",
                                disabled=not check_database_exists())

    if user_question:
        if check_database_exists():
            with st.spinner("ğŸ¤” AIæ­£åœ¨åˆ†ææ–‡çŒ®ï¼Œè¯·ç¨å€™..."):
                user_input(user_question)
        else:
            st.error("âŒ è¯·å…ˆä¸Šä¼ å¹¶å¤„ç†PDFæ–‡ä»¶ï¼")

    # Display related papers if they exist in the session state
    if "related_papers" in st.session_state and st.session_state.related_papers:
        st.markdown("---")
        st.subheader("ğŸ” ç›¸å…³æ–‡çŒ®æ¨è")
        
        with st.expander("ç‚¹å‡»æŸ¥çœ‹æ¨èçš„5ç¯‡ç›¸å…³æ–‡çŒ®", expanded=True):
            papers = st.session_state.related_papers
            if isinstance(papers, list) and papers:
                for i, paper in enumerate(papers):
                    if "error" in paper:
                        st.error(f"åœ¨æ£€ç´¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {paper['error']}")
                        continue

                    st.markdown(f"**{i+1}. {paper.get('title', 'N/A')}**")
                    st.markdown(f"_ä½œè€…: {paper.get('authors', 'N/A')} ({paper.get('year', 'N/A')})_")
                    st.markdown(f"**æ‘˜è¦**: {paper.get('abstract', 'æ— å¯ç”¨æ‘˜è¦')}")
                    st.markdown(f"[ğŸ”— é˜…è¯»åŸæ–‡]({paper.get('url', '#')})")
                    if i < len(papers) - 1:
                        st.markdown("---")
            else:
                st.write("æœªèƒ½æ‰¾åˆ°ç›¸å…³çš„æ–‡çŒ®ï¼Œæˆ–è¿”å›æ ¼å¼ä¸æ­£ç¡®ã€‚")

    # ä¾§è¾¹æ 
    with st.sidebar:
        st.title("ğŸ“š æ–‡æ¡£åº“ç®¡ç†")
        
        # æ˜¾ç¤ºå½“å‰çŠ¶æ€
        if check_database_exists():
            st.success("âœ… çŠ¶æ€ï¼šçŸ¥è¯†åº“å·²å°±ç»ª")
        else:
            st.info("â„¹ï¸ çŠ¶æ€ï¼šç­‰å¾…ä¸Šä¼ æ–‡çŒ®")
        
        st.markdown("---")
        
        # æ–‡ä»¶ä¸Šä¼ 
        pdf_docs = st.file_uploader(
            "ğŸ“ ä¸Šä¼ PDFæ ¼å¼çš„å­¦æœ¯æ–‡çŒ®", 
            accept_multiple_files=True,
            type=['pdf'],
            help="æ”¯æŒä¸€æ¬¡æ€§ä¸Šä¼ å¤šç¯‡ç›¸å…³æ–‡çŒ®è¿›è¡Œç»¼åˆåˆ†æ"
        )
        
        if pdf_docs:
            st.info(f"ğŸ“„ å·²é€‰æ‹© {len(pdf_docs)} ç¯‡æ–‡çŒ®")
            for i, pdf in enumerate(pdf_docs, 1):
                st.write(f"   {i}. {pdf.name}")
        
        st.markdown("---")

        # å¤„ç†æŒ‰é’®
        process_button = st.button(
            "ğŸš€ æ„å»ºçŸ¥è¯†åº“", 
            disabled=not pdf_docs,
            use_container_width=True
        )

        if process_button:
            if "related_papers" in st.session_state:
                del st.session_state.related_papers  # Clear previous recommendations
            if pdf_docs:
                with st.spinner("ğŸ“Š æ­£åœ¨å¤„ç†æ–‡çŒ®ï¼Œè¯·ç¨å€™..."):
                    try:
                        # æ–°çš„å¤„ç†æµç¨‹
                        chunk_count = process_pdfs(pdf_docs)
                        st.info(f"âœ… æ–‡çŒ®å·²æˆåŠŸå¤„ç†ï¼Œåˆ‡åˆ†ä¸º {chunk_count} ä¸ªçŸ¥è¯†ç‰‡æ®µã€‚")
                        st.success("ğŸ‰ çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼ç°åœ¨å¯ä»¥å¼€å§‹æé—®äº†ã€‚")
                        st.balloons()
                        time.sleep(1.5) # æš‚åœ1.5ç§’
                        st.rerun()
                        
                    except Exception as e:
                        st.error(f"âŒ å¤„ç†PDFæ—¶å‡ºé”™: {str(e)}")
            else:
                st.warning("âš ï¸ è¯·å…ˆé€‰æ‹©è¦ä¸Šä¼ çš„PDFæ–‡ä»¶")
        
        # æ–°å¢ï¼šç›¸å…³æ–‡çŒ®æ¨èåŠŸèƒ½
        st.markdown("---")
        st.subheader("ğŸ”— ç›¸å…³æ–‡çŒ®æ¨è")

        if pdf_docs:
            pdf_options = {pdf.name: pdf for pdf in pdf_docs}
            selected_pdf_name = st.selectbox(
                "è¯·é€‰æ‹©ä¸€ç¯‡è®ºæ–‡ä½œä¸ºåˆ†æåŸºç¡€ï¼š",
                options=list(pdf_options.keys())
            )
            
            find_related_button = st.button(
                "æŸ¥æ‰¾ç›¸å…³æ–‡çŒ®",
                use_container_width=True
            )

            if find_related_button and selected_pdf_name:
                selected_pdf_file = pdf_options[selected_pdf_name]
                if "related_papers" in st.session_state:
                     del st.session_state.related_papers # Clear previous results before new search
                run_related_papers_agent(selected_pdf_file)
                st.rerun()
        else:
            st.info("è¯·å…ˆä¸Šä¼ PDFæ–‡ä»¶ä»¥å¯ç”¨æ­¤åŠŸèƒ½ã€‚")


        # ä½¿ç”¨è¯´æ˜
        with st.expander("ğŸ’¡ ä½¿ç”¨æŒ‡å—"):
            st.markdown("""
            **åˆ†ææµç¨‹ï¼š**
            1. ğŸ“ **ä¸Šä¼ æ–‡çŒ®**ï¼šåœ¨ä¸Šæ–¹ä¸Šä¼ ä¸€ç¯‡æˆ–å¤šç¯‡PDFæ ¼å¼çš„å­¦æœ¯æ–‡çŒ®ã€‚
            2. ğŸš€ **æ„å»ºçŸ¥è¯†åº“**ï¼šç‚¹å‡»â€œæ„å»ºçŸ¥è¯†åº“â€æŒ‰é’®ï¼ŒAIå°†å¯¹æ–‡çŒ®è¿›è¡Œæ·±åº¦è§£æå’Œç´¢å¼•ã€‚
            3. ğŸ’¬ **å¼€å§‹æé—®**ï¼šåœ¨ä¸»ç•Œé¢çš„è¾“å…¥æ¡†ä¸­ï¼Œé’ˆå¯¹æ–‡çŒ®å†…å®¹æå‡ºæ‚¨çš„é—®é¢˜ã€‚
            4. ğŸ¤– **è·å–æ´å¯Ÿ**ï¼šAIå°†åŸºäºæ–‡çŒ®å†…å®¹ï¼Œä¸ºæ‚¨æä¾›ç²¾å‡†ã€ç»¼åˆçš„å›ç­”ã€‚
            
            **é«˜çº§æŠ€å·§ï¼š**
            - **å¤šæ–‡ä»¶åˆ†æ**ï¼šå¯åŒæ—¶ä¸Šä¼ å¤šç¯‡ç›¸å…³ä¸»é¢˜çš„è®ºæ–‡ï¼Œè¿›è¡Œè·¨æ–‡çŒ®çš„ç»¼åˆé—®ç­”ã€‚
            - **æ¸…é™¤çŸ¥è¯†åº“**ï¼šå½“æ‚¨æƒ³åˆ†ææ–°çš„ä¸»é¢˜æ—¶ï¼Œå¯ä»¥æ¸…é™¤æ—§çš„çŸ¥è¯†åº“ï¼Œé‡æ–°å¼€å§‹ã€‚
            """)
            
        st.markdown("---")
        st.info("åŠ©æ‰‹ by Zhang.E.B")

if __name__ == "__main__":
    main()
